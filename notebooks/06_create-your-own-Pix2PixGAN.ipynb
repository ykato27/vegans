{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b0a64e",
   "metadata": {},
   "source": [
    "# Creating your own GAN II: Pix2Pix\n",
    "\n",
    "In the last notebook we implemented the `LSGAN` together, this time we're tackling the [Pix2PixGAN](https://arxiv.org/pdf/1611.07004.pdf). Check also [this resource](https://blog.eduonix.com/artificial-intelligence/pix2pix-gan/) for a good explanation. As the time of writing this notebook (2021-04-08 19:14) there are only 4 (8) GAN architectures implemented in `vegans`: `VanillaGAN`, `WasssersteinGAN`, `WassersteinGANGP`, `LSGAN` and all there conditional variants. The Pix2Pix GAN is a purely conditional network, which (as the name suggests) is popular for image to image translation tasks (rotating mnist digits, summer to winter scenery, horses to zebras, person to person with beard, ...). Note that we could use conditional WassersteinGANs for this task but they are not really optimized for it, so we will implement this algorithm here.\n",
    "\n",
    "We suppose you have read the previous notebook on the creation of the `LSGAN`. If not, please go over it so you have a understanding of the abstract base classes. We will not present it here and jump basically right into the implementation.\n",
    "\n",
    "First import the usual libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52704b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from vegans.GAN import ConditionalWassersteinGAN, ConditionalWassersteinGANGP\n",
    "from vegans.utils.utils import plot_losses, plot_images, get_input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56225be4",
   "metadata": {},
   "source": [
    "## ConditionalPix2Pix\n",
    "\n",
    "As we mentioned in the introduction, there is not really an unconditional version of the Pix2Pix GANs because you always need data and the corresponding transformed image (the label in a way). But we are still choosing this name to be consistent with the naming schema of the `vegans` library (if it takes `y_dim` as input name it `conditional`). The following graphic is taken from the BicycleGAN paper:\n",
    "\n",
    "![Pix2PixGAN](./picts/Pix2PixSchema.png)\n",
    "\n",
    "We already know that we somehow need to inherit from the `AbstractConditionalGenerativeModel` base class. However, if you are familiar with the Pix2Pix algorithm you'll know that it consists of two networks: a generator and a discriminator. So from the last notebook tutorial we know that we should be able to inherit from the `ConditioanlAbstractGAN1v1`. Let's try that (if this sentence is still in the notebook while you're reading it probably means it worked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "590e117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vegans.models.conditional.AbstractConditionalGAN1v1 import AbstractConditionalGAN1v1\n",
    "from torch.nn import BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1166c62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalPix2Pix(AbstractConditionalGAN1v1):\n",
    "    def __init__(\n",
    "            self,\n",
    "            generator,\n",
    "            adversary,\n",
    "            x_dim,\n",
    "            z_dim,\n",
    "            y_dim,\n",
    "            optim=None,\n",
    "            optim_kwargs=None,\n",
    "            feature_layer=None,\n",
    "            fixed_noise_size=32,\n",
    "            device=None,\n",
    "            folder=\"./ConditionalPix2Pix\",\n",
    "            ngpu=None):\n",
    "\n",
    "        super().__init__(\n",
    "            generator=generator, adversary=adversary,\n",
    "            x_dim=x_dim, z_dim=z_dim, y_dim=y_dim, adv_type=\"Discriminator\",\n",
    "            optim=optim, optim_kwargs=optim_kwargs, feature_layer=feature_layer,\n",
    "            fixed_noise_size=fixed_noise_size,\n",
    "            device=device, folder=folder, ngpu=ngpu\n",
    "        )\n",
    "\n",
    "    def _define_loss(self):\n",
    "        self.loss_functions = {\"Generator\": BCELoss(), \"Adversary\": BCELoss()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f9db43",
   "metadata": {},
   "source": [
    "Note that this is not the whole story. If we leave it like that it would be a normal `VanillaGAN` (original GAN implementation). The Pix2Pix GAN introduces a pixel-wise loss for the generator. So the goal for the generator is twofold:\n",
    "\n",
    "   1. Try to fool the discriminator and force it to output 1 for fake images (classify them as real).\n",
    "   2. Minimize the pixel-wise mean squared error between the generated and target image.\n",
    "   \n",
    "So far we have only covered the first part so let's implement the pixel-wise loss. We can do this by creating a hook into the `AbstractConditionalGAN1v1` implementation. During training this parent class executes the following code snippet during every training step:\n",
    "\n",
    "```python\n",
    "def calculate_losses(self, X_batch, Z_batch, y_batch, who=None):\n",
    "    if who == \"Generator\":\n",
    "        self._calculate_generator_loss(X_batch=X_batch, Z_batch=Z_batch, y_batch=y_batch)\n",
    "    elif who == \"Adversary\":\n",
    "        self._calculate_adversary_loss(X_batch=X_batch, Z_batch=Z_batch, y_batch=y_batch)\n",
    "    else:\n",
    "        self._calculate_generator_loss(X_batch=X_batch, Z_batch=Z_batch, y_batch=y_batch)\n",
    "        self._calculate_adversary_loss(X_batch=X_batch, Z_batch=Z_batch, y_batch=y_batch)\n",
    "        self._losses[\"Loss/LossRatio\"] = self._losses[\"Adversary_real\"]/self._losses[\"Adversary_fake\"]\n",
    "```\n",
    "\n",
    "where `who` is either \"Generator\" or \"Adversary\" (Sometimes **OUTSIDE** of training it might be `None`. This is used for calculating losses which are saved, logged and printed to the console but never for training!). So we can create our own computation for the generator loss by defining our own method for \n",
    "`self._calculate_generator_loss(Z_batch=Z_batch, y_batch=y_batch)`. The \"original\" implementation for this method within `AbstractConditionalGAN1v1` looks like this:\n",
    "\n",
    "```python\n",
    "def _calculate_generator_loss(self, X_batch, Z_batch, y_batch):\n",
    "    fake_images = self.generate(y=y_batch, z=Z_batch)\n",
    "    fake_predictions = self.predict(x=fake_images, y=y_batch)\n",
    "    gen_loss = self.loss_functions[\"Generator\"](\n",
    "        fake_predictions, torch.ones_like(fake_predictions, requires_grad=False)\n",
    "    )\n",
    "    self._losses.update({\"Generator\": gen_loss})\n",
    "```\n",
    "\n",
    "So we reuse what we can but also include a pixel-wise image loss. Note that we include an additonal parameter to weight the pixel-wise loss called `lambda_x`. The appropriate place to introduce this parameter is in the constructor. You do not have to include this parameter in self.hyperparameters (which is created by `AbstractGenerativeModel` of course), but it's nice to have all these values in a centralized dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "331c9b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalPix2Pix(AbstractConditionalGAN1v1):\n",
    "    def __init__(\n",
    "            self,\n",
    "            generator,\n",
    "            adversary,\n",
    "            x_dim,\n",
    "            z_dim,\n",
    "            y_dim,\n",
    "            optim=None,\n",
    "            optim_kwargs=None,\n",
    "            lambda_x = 10,\n",
    "            fixed_noise_size=32,\n",
    "            device=None,\n",
    "            folder=\"./ConditionalPix2Pix\",\n",
    "            ngpu=None):\n",
    "\n",
    "        super().__init__(\n",
    "            generator=generator, adversary=adversary,\n",
    "            x_dim=x_dim, z_dim=z_dim, y_dim=y_dim, adv_type=\"Discriminator\",\n",
    "            optim=optim, optim_kwargs=optim_kwargs,\n",
    "            fixed_noise_size=fixed_noise_size,\n",
    "            device=device, folder=folder, ngpu=ngpu\n",
    "        )\n",
    "        self.lambda_x = 10\n",
    "        self.hyperparameters[\"lambda_x\"] = self.lambda_x\n",
    "\n",
    "    def _define_loss(self):\n",
    "        self.loss_functions = {\"Generator\": BCELoss(), \"Adversary\": BCELoss(), \"L1\": torch.nn.L1Loss()}\n",
    "        \n",
    "    def _calculate_generator_loss(self, X_batch, Z_batch, y_batch):\n",
    "        fake_images = self.generate(y=y_batch, z=Z_batch)\n",
    "        if self.feature_layer is None:\n",
    "            fake_predictions = self.predict(x=fake_images, y=y_batch)\n",
    "            gen_loss_original = self.loss_functions[\"Generator\"](\n",
    "                fake_predictions, torch.ones_like(fake_predictions, requires_grad=False)\n",
    "            )\n",
    "        else:\n",
    "            gen_loss_original = self._calculate_feature_loss(X_real=X_batch, X_fake=fake_images, y_batch=y_batch)\n",
    "        gen_loss_pixel_wise = self.loss_functions[\"L1\"](\n",
    "            X_batch, fake_images\n",
    "        )\n",
    "        gen_loss = gen_loss_original + self.lambda_x*gen_loss_pixel_wise\n",
    "        self._losses.update({\n",
    "            \"Generator\": gen_loss,\n",
    "            \"Generator_Original\": gen_loss_original,\n",
    "            \"Generator_L1\": gen_loss_pixel_wise\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803e2f6d",
   "metadata": {},
   "source": [
    "Note that we have included multiple losses in the `self._losses` dictionary. The only necessary one is with the key \"Generator\" because we have a network with this name in the `self.neural_nets` dictionary (the other one is called \"Adversary\" for GANs inheriting from `AbstractConditionalGAN1v1`). No backward step is performed on the other two but they are logged within tensorboard and the internal `self.logged_losses` dictionary.\n",
    "\n",
    "The logical `if` statement allows for the usage of a feature loss instead of the usual cross-entropy. This might help training by not training the generator to output a label of one for fake images but by matching an intermediate feature layer of the discriminator so that it is equal for real and fake samples. This would of course subsequently lead to the same prediction output value.\n",
    "\n",
    "**IMPORTANT NOTE**: If you are familiar with the [Pix2Pix Paper](https://arxiv.org/pdf/1611.07004.pdf) or other implementations of this algorithm you might be confused why we still have a Z_batch in there. The paper itself claims that the noise isn't really useful in the modeling here. Unfortuantely this is maybe one of the drawbacks of `vegans` (at least currently). With decreased ease of implementation and greater generalization we can not take care of every special use case. \n",
    "We still recommend using a very small dimension for `z_dim` so it doesn't greatly improve computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f674ff",
   "metadata": {},
   "source": [
    "Please again note that this is a prelimanary tutorial implementation which might or might not change in future releases of `vegans`. So this implementation might not be completely up-to-date, but still is a viable implementation nonetheless."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

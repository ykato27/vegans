{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Example\n",
    "In this notebook we'll use the `vegans` library to generate some fake handwritten digits, using a generator and discriminator of our choice. \n",
    "\n",
    "### Initial Setup\n",
    "First, some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import vegans.utils.loading as loading\n",
    "from vegans.GAN import WassersteinGAN, WassersteinGANGP\n",
    "from vegans.utils.utils import plot_losses, plot_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we have cuda?\n",
    "print('Cuda is available: {}'.format(torch.cuda.is_available()))\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now download the mnist dataset and set the parameters below (To get exactly the same format as in this tutorial download from [here](https://github.com/tneuer/GAN-pytorch/tree/main/data/mnist), but of course you can load it from anywhere you want):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This directory will contain the MNIST data\n",
    "datapath = \"./data\"\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "z_dim = [1, 4, 4]\n",
    "\n",
    "# Input channels \n",
    "nc = 1\n",
    "\n",
    "# Padding for mnist images (28x28) -> (32x32)\n",
    "pad = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load and preprocess the data:\n",
    "- The images are saved in gray scale from 0-255, so we scale it to 0-1. Then we can use a Sigmoid as the last layer of the generator.\n",
    "- The original image shape is (28, 28) but when working with convolutional layers it is often beneficial to have a power of two. Therefore we pad two empty rows and columns to every image.\n",
    "- Finally we reshape the images because we need the images in the shape of (nr_channels, nr_heiht_pixels, nr_width_pixels). In out case this results in [1, 32, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create dataset\n",
    "\"\"\"\n",
    "X_train, y_train, X_test, y_test = loading.load_data(datapath, which=\"mnist\", download=True)\n",
    "\n",
    "X_train = X_train.reshape((-1, 1, 32, 32))\n",
    "X_test = X_test.reshape((-1, 1, 32, 32))\n",
    "X_train = X_train / np.max(X_train)\n",
    "X_test = X_test / np.max(X_test)\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "x_dim = X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some of the training images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plot_images(images=X_train.reshape(-1, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of Generator and Discriminator / Critic\n",
    "We'll specify the architecture of the generator and discriminator / critic networks. It's difficult to know which architectures to choose before training. Here we used a architecture which proved to work.\n",
    "\n",
    "Since we want to train a Wasserstein GAN, the output of the critic should be a real number and not a probability. Therefore we drop the last sigmoid and use the identity function. If you want to switch to a architecture that uses a discriminator switch the `nn.Identity` with `nn.Sigmoid` for the adversary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Generator\n",
    "\"\"\"\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        ngf = 20\n",
    "        self.hidden_part = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=z_dim[0], out_channels=ngf * 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(ngf * 2, ngf * 2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(ngf, ngf, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, nc, 5, 1, 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(nc, nc, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_part(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "\"\"\" Adversary\n",
    "\"\"\"\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        ncf = 8\n",
    "        self.hidden_part = nn.Sequential(\n",
    "            # input is (nc) x 32 x 32\n",
    "            nn.Conv2d(in_channels=nc, out_channels=ncf, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ncf) x 16 x 16\n",
    "            nn.Conv2d(ncf, ncf * 2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ncf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ncf*2) x 8 x 8\n",
    "            nn.Conv2d(ncf * 2, ncf * 4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ncf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ncf*4) x 4 x 4\n",
    "            nn.Conv2d(ncf * 4, ncf * 8, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ncf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ncf*8) x 2 x 2\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=ncf*8*2*2, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=1)\n",
    "        )\n",
    "        self.output = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_part(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "generator = Generator()\n",
    "critic = Critic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our GAN\n",
    "Build a Wasserstein GAN trainer, using default optimizers (we can also specify our own). To use a different GAN algorithm, just use the corresponding class (e.g., `VanillaGAN` for original GAN).\n",
    "\n",
    "Here you can specify some optional GAN parameters, such as the latent space dimension `z_dim`, the number of samples to save (`fixed_noise_size`) and the optimizer keyword arguments (`optim_kwargs`). We set `folder=None` so that no folder is created where all results would be stored. Otherwise we could give a path like `folder=\"TrainedModels/GAN\"`. All results (summary, images, loss functions, tensorboard information, models) would be saved in that folder. You can control what should be saved in the `fit` method. This folder will never overwrite an existing folder. If the path already exists a new path of the form `folder=path_{TimeStamp}` is created.\n",
    "\n",
    "We also decrease the learning rate of the critic a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_kwargs = {\"Generator\": {\"lr\": 0.0005}, \"Adversary\": {\"lr\": 0.0001}}\n",
    "gan = WassersteinGAN(\n",
    "    generator, critic, z_dim=z_dim, x_dim=x_dim, \n",
    "    optim_kwargs=optim_kwargs, fixed_noise_size=20, folder=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the networks by calling the `fit()` method. Here you can specify some parameters for training like `eochs`, `batch_size`, `save_model_every`, `save_images_every`, `print_every`, `enable_tensorboard` and others.\n",
    "\n",
    "You can interrupt training at any time and still access train stats from within the `gan` object. You can resume training later. Note that we increase the number of steps the critic (adversary) is trained, which is common for Wasserstein GANs but not VanillaGANs so take care when switching out algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = {\"Adversary\": 5}\n",
    "gan.fit(\n",
    "    X_train, epochs=5, steps=steps,\n",
    "    print_every=\"0.25e\", save_losses_every=10, enable_tensorboard=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, losses = gan.get_training_results()\n",
    "fig, axs = plot_losses(losses)\n",
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plot_images(samples.reshape(-1, 32, 32), n=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to generate new images and have control over the number of generated images. Note that the `get_training_results` returns as many images as were specified with the `fixed_noise_size` argument in the constructor when creating the GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples = gan.generate(n=100)\n",
    "print(new_samples.shape)\n",
    "fig, axs = plot_images(samples.reshape(-1, 32, 32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
